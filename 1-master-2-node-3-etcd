Vagrantfile:

```bash
# -*- mode: ruby -*-
# vi: set ft=ruby :
IMAGE_NAME = "ubuntu/xenial64"


Vagrant.configure("2") do |config|
    config.ssh.insert_key = false

    config.vm.provider "virtualbox" do |v|
        v.memory = 1024
        v.cpus = 1
    end

    config.vm.define "k8s-master" do |master|
        master.vm.box = IMAGE_NAME
        master.vm.network "private_network", ip: "192.168.50.10"
        master.vm.hostname = "k8s-master"
    end

    (1..2).each do |i|
        config.vm.define "node-#{i}" do |node|
            node.vm.box = IMAGE_NAME
            node.vm.network "private_network", ip: "192.168.50.#{i + 10}"
            node.vm.hostname = "node-#{i}"
        end
    end

    (1..3).each do |i|
        config.vm.define "etcd-#{i}" do |etcd|
            etcd.vm.box = IMAGE_NAME
            etcd.vm.network "private_network", ip: "192.168.50.#{i + 20}"
            etcd.vm.hostname = "etcd-#{i}"
        end
    end

end
```
Setup:

```
master: 192.168.50.10
node-1: 192.168.50.11
node-2:  192.168.50.12
etcd-1 : 192.168.50.21
etcd-2 : 192.168.50.22
etcd-3 : 192.168.50.23
```

Prerequisite (all nodes):

```
apt-get update
apt-get install docker.io -y
systemctl enable docker && systemctl start docker
apt-get install apt-transport-https curl -y
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add
apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
apt-get update && apt-get install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet
swapoff -a
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
```

etcd nodes

```
cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
ExecStart=
ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet
```

etcd-1

```
root@etcd-1:~# export HOST0=192.168.50.21
root@etcd-1:~# export HOST1=192.168.50.22
root@etcd-1:~# export HOST2=192.168.50.23

root@etcd-1:~# mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/
root@etcd-1:~# ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
root@etcd-1:~# NAMES=("infra0" "infra1" "infra2")

root@etcd-1:~# for i in "${!ETCDHOSTS[@]}"; do

HOST=${ETCDHOSTS[$i]}
NAME=${NAMES[$i]}

cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
apiVersion: "kubeadm.k8s.io/v1beta1"
kind: ClusterConfiguration
etcd:
    local:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
            initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
            initial-cluster-state: new
            name: ${NAME}
            listen-peer-urls: https://${HOST}:2380
            listen-client-urls: https://${HOST}:2379
            advertise-client-urls: https://${HOST}:2379
            initial-advertise-peer-urls: https://${HOST}:2380
EOF

done

root@etcd-1:~# apt-get install tree -y
root@etcd-1:~# kubeadm init phase certs etcd-ca

### Create certificates for the etcd-3 node
root@etcd-1:~# kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
root@etcd-1:~# kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
root@etcd-1:~# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
root@etcd-1:~# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
root@etcd-1:~# cp -R /etc/kubernetes/pki /tmp/${HOST2}/
root@etcd-1:~# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

### Create certificates for the etcd-2 node
root@etcd-1:~# kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
root@etcd-1:~# kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
root@etcd-1:~# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
root@etcd-1:~# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
root@etcd-1:~# cp -R /etc/kubernetes/pki /tmp/${HOST1}/
root@etcd-1:~# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

### Create certificates for the etcd-1 node
root@etcd-1:~# kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
root@etcd-1:~# kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
root@etcd-1:~# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
root@etcd-1:~# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml

root@etcd-1:~# find /tmp/${HOST2} -name ca.key -type f -delete
root@etcd-1:~# find /tmp/${HOST1} -name ca.key -type f -delete

### Copy certificates and kubeadm configs to the etcd-2 and etcd-3 nodes.
### First generate a ssh key pair (ssh-keygen -t rsa) on etcd-1 and add public part to the etcd-2 & 3 nodes.

root@etcd-1:~# scp -r /tmp/${HOST1}/* ${HOST1}:
root@etcd-1:~# scp -r /tmp/${HOST2}/* ${HOST2}:

root@etcd-1:~# ssh ${HOST1}
root@etcd-2# cd /root
root@etcd-2# mv pki /etc/kubernetes/
root@etcd-2# apt-get install tree -y
root@etcd-2# exit

root@etcd-1:~# ssh ${HOST2}
root@etcd-3:~# cd /root
root@etcd-3:~# mv pki /etc/kubernetes/
root@etcd-3:~# apt-get install tree -y
root@etcd-3:~# exit

root@etcd-1:~# kubeadm init phase etcd local --config=/tmp/192.168.50.21/kubeadmcfg.yaml
root@etcd-1:~# ssh ${HOST1}
root@etcd-2:~# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml
root@etcd-2:~# exit

root@etcd-1:~# ssh ${HOST2}
root@etcd-3:~# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml
root@etcd-3:~# exit

### Check cluster status

root@etcd-1:~# docker run --rm -it \
--net host \
-v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.24 etcdctl \
--cert-file /etc/kubernetes/pki/etcd/peer.crt \
--key-file /etc/kubernetes/pki/etcd/peer.key \
--ca-file /etc/kubernetes/pki/etcd/ca.crt \
--endpoints https://192.168.50.21:2379 cluster-health

### status output
member 6311727a8df181c7 is healthy: got healthy result from https://192.168.50.23:2379
member 6b85f157810fe4ab is healthy: got healthy result from https://192.168.50.21:2379
member 834444e7d46c33e4 is healthy: got healthy result from https://192.168.50.22:2379


### First copy etcd-1 public key to master (ip: 192.168.50.10)

root@etcd-1:~# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.50.10:
root@etcd-1:~# scp /etc/kubernetes/pki/apiserver-etcd-client.crt 192.168.50.10:
root@etcd-1:~# scp /etc/kubernetes/pki/apiserver-etcd-client.key 192.168.50.10:
```

Master Node (k8s-master)

```
root@k8s-master:~# cd /root && vim kubeadm-config.yaml

apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
kubernetesVersion: stable
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.244.0.0/16
controlPlaneEndpoint: 192.168.50.10:6443
apiServer:
  certSANs:
  - 192.168.50.10
etcd:
    external:
        endpoints:
        - https://192.168.50.21:2379
        - https://192.168.50.22:2379
        - https://192.168.50.23:2379
        caFile: /etc/kubernetes/pki/etcd/ca.crt
        certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
        keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
		

root@k8s-master:~# mkdir -p /etc/kubernetes/pki/etcd
root@k8s-master:~# cp /root/ca.crt /etc/kubernetes/pki/etcd/
root@k8s-master:~# cp /root/apiserver-etcd-client.crt /etc/kubernetes/pki/
root@k8s-master:~# cp /root/apiserver-etcd-client.key /etc/kubernetes/pki/

root@k8s-master:~# kubeadm init --config kubeadm-config.yaml

[ERROR NumCPU]: the number of available CPUs 1 is less than the required 2

root@k8s-master:~# kubeadm init --config kubeadm-config.yaml --ignore-preflight-errors=NumCPU

### Important output
kubeadm join 192.168.50.10:6443 --token hvzy19.ji5anqwu66jl50ww \
    --discovery-token-ca-cert-hash sha256:a2479a35fb18dec5764ed675c5795b263b58480813ecce3f45efc9169f78f26f

root@k8s-master:~# vim kube-flannel.yml

---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: psp.flannel.unprivileged
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default
    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  volumes:
    - configMap
    - secret
    - emptyDir
    - hostPath
  allowedHostPaths:
    - pathPrefix: "/etc/cni/net.d"
    - pathPrefix: "/etc/kube-flannel"
    - pathPrefix: "/run/flannel"
  readOnlyRootFilesystem: false
  # Users and groups
  runAsUser:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  # Privilege Escalation
  allowPrivilegeEscalation: false
  defaultAllowPrivilegeEscalation: false
  # Capabilities
  allowedCapabilities: ['NET_ADMIN']
  defaultAddCapabilities: []
  requiredDropCapabilities: []
  # Host namespaces
  hostPID: false
  hostIPC: false
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  # SELinux
  seLinux:
    # SELinux is unsed in CaaSP
    rule: 'RunAsAny'
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
rules:
  - apiGroups: ['extensions']
    resources: ['podsecuritypolicies']
    verbs: ['use']
    resourceNames: ['psp.flannel.unprivileged']
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-amd64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: amd64
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: KUBERNETES_SERVICE_HOST
          value: "192.168.50.10"
        - name: KUBERNETES_SERVICE_PORT
          value: "6443"
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-arm64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: arm64
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.11.0-arm64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-arm64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-arm
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: arm
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.11.0-arm
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-arm
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-ppc64le
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: ppc64le
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.11.0-ppc64le
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-ppc64le
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-s390x
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: s390x
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.11.0-s390x
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-s390x
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
			
root@k8s-master:~# kubectl apply -f kube-flannel.yml
## https://github.com/kubernetes/kubernetes/issues/39701

root@k8s-master:~# kubectl --kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -owide
root@k8s-master:~# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes -owide
root@k8s-master:~# mkdir -p $HOME/.kube
root@k8s-master:~# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
root@k8s-master:~# chown $(id -u):$(id -g) $HOME/.kube/config
root@k8s-master:~# kubectl get cs
```

Worker Nodes (node-1,2)

```
vagrant@node-1:~$ sudo kubeadm join 192.168.50.10:6443 --token hvzy19.ji5anqwu66jl50ww \
    --discovery-token-ca-cert-hash sha256:a2479a35fb18dec5764ed675c5795b263b58480813ecce3f45efc9169f78f26f
```

Comments

```
Vagrant typically assigns two interfaces to all VMs. The first, for which all hosts are assigned the IP address 10.0.2.15, is for external traffic that gets NATed.
This may lead to problems with flannel. By default, flannel selects the first interface on a host. This leads to all hosts thinking they have the same public IP address. 

kubectl exec return error: unable to upgrade connection: pod does not exist
Use the /etc/systemd/system/kubelet.service.d/10-kubeadm.conf file, to append the IP of the node/worker
Environment="KUBELET_EXTRA_ARGS=--node-ip=192.168.50.XX"
```

kubernetes-dashboard

Make sure the kubernetes-dashboard is running on the master node. 

```
# kubectl taint nodes --all node-role.kubernetes.io/master-
```

Do not shedule pod on master:

```
$ kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule-

$ kubectl describe no k8s-master | grep -i taint
Taints:             node-role.kubernetes.io/master:NoSchedule
```

As you can see, this node has a taint node-role.kubernetes.io/master:NoSchedule. Pods are not going to allow to schedule on Master node

The taint has the key node-role.kubernetes.io/master, value nil (which is not shown), and taint effect NoSchedule.

Untaint the setting on master

```
$ kubectl taint nodes k8s-master node-role.kubernetes.io/master-
$ kubectl describe nodes k8s-master | grep Taints
Taints:             <none>

$ kubectl taint nodes k8s-master node-role.kubernetes.io/master="":NoSchedule

$ kubectl describe nodes k8s-master | grep Taints
Taints:             node-role.kubernetes.io/master:NoSchedule
```

you can use tolerations to override NoSchedule taints

```
$ wget https://k8s.io/examples/controllers/nginx-deployment.yaml
```

Add this to your pod’s spec:

```
    spec:
      containers:
      - name: nginx
....
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
		
```

Make sure the kubernetes-dashboard is running on the master node.

https://github.com/kubernetes/kubernetes/issues/29540

````
root@k8s-master:~# wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml

root@k8s-master:~# kubectl label nodes k8s-master dedicated=master
node/k8s-master labeled

````
Edit kubernetes-dashboard.yaml and add this to your pod’s spec:

```
      nodeSelector:
        dedicated: master
````

Your api server is exposed but not accessible from outside. You can not access any kubernetes endpoints if you do not authenticate and authorize yourself.

```
$grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.crt

grep 'client-key-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.key

openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-client"
```

Import the kubecfg.p12 certificate in firefox and open

```
https://192.168.50.10:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
```

Accept any warning and you should see the authentication page. You can skip the login and check you are not able to perform any task.

Create service account 

```
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
EOF
```

Create ClusterRoleBinding 

```
cat <<EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system
EOF
```

Get the Bearer Token. Once you run the following command, copy the token value which you will use on the following step. 

```
$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
```

Come back to your browser and choose token on the login page. You will need to paste the token value you have copied on the previous step.
